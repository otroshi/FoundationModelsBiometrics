# Foundation Models and Biometrics

<a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.174119169.94570936" target="_blank">
  <img src="https://img.shields.io/badge/TechRxiv-002855.svg?logo=ieee" alt="TechRxiv">
</a>
<a href="https://forms.gle/NYNjhEKg6q4Pn1UM7" target="_blank">
  <img src="https://img.shields.io/badge/Maintained%3F-yes-green.svg" alt="Maintenance">
</a>


## ðŸš€ About the Survey

This paper provides an overview of the recent advancements in foundation models and discusses potential applications of these models in the field of *biometrics*. **Foundation models**, such as **Large Language Models (LLMs)**, **Vision Language Models (VLMs)**, **Audio-Language Models (ALMs)**, and **Large Multi-modal Models (LMMs)**, are based on large neural networks which are trained with massive amounts of data and enable robust feature extraction for transfer learning. These models allow efficient zero-shot and few-shot learning, achieving state-of-the-art performance in downstream tasks.
Biometrics is also an active field of research, which involves various research problems, ranging from robust recognition to security and privacy in biometric systems. In this paper, we present an in-depth analysis of state-of-the-art methodologies regarding foundation multi-modal models, their advancements, and their applicability to biometrics tasks. We also highlight current limitations and provide insights into potential future research directions in the applications of foundation models in biometrics. To our knowledge, this paper is the **first survey** which investigates the *applications of foundation models in biometrics*.
[<a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.174119169.94570936" target="_blank">
  Link to pre-print
</a>]

The survey is structured as follows for clarity and readability:

- **Foundation Models**:
In this section, we review recent advancements in foundation models and mention state-of-the-art models. We catgeorise foundation models in four different catgories:
  - **Large Language Models (LLMs)**
  - **Vision Language Models (VLMs)**
  - **Audio-Language Models (ALMs)**
  - **Large Multi-modal Models (LMMs)**

- **Biometric Recognition and Security**:
In this section, we review the general pipeline of biometric systems. We describe attack points in biometric systems and discuss security and pprivacy threats.

- **Applications of Foundation Models in Biometrics**:
In this section, we review recent work on the applications of foundation models in biometrics:
  - **Foundation Models for Biometric Recognition**
  - **Foundation Models for Soft-biometric Detection**
  - **Foundation Models for Deepfake and Forgery Detection**
  - **Foundation Models for Anti-spoofing**
  - **Foundation Models for Synthetic Biometric Generation**



## ðŸ“š Missing Papers

We will keep updating this survey and this git repository. Please contact the first author (hatef.otroshi@idiap.ch) or complete the following form to submit your paper for citation.  

ðŸ‘‰ [Submit Your Paper](https://forms.gle/NYNjhEKg6q4Pn1UM7)

We appreciate your contributions and look forward to keeping this survey comprehensive and up to date!

## Foundation Models

### Large Language Models (LLMs)

<details>
TODO
</details>

### Vision Language Models (VLMs)

<details>
TODO
</details>

### Audio-Language Models (ALMs)

<details>
TODO
</details>

### Large Multi-modal Models (LMMs)

<details>
TODO
</details>



## Applications of Foundation Models in Biometrics

### Foundation Models for Biometric Recognition

<details>
TODO
</details>

### Foundation Models for Soft-biometric Detection

<details>
TODO
</details>

### Foundation Models for Deepfake and Forgery Detection

<details>
TODO
</details>

### Foundation Models for Anti-spoofing

<details>
TODO
</details>

### Foundation Models for Synthetic Biometric Generation
<details>
TODO
</details>


## Citation

```bibtex
@article{fmbiometrics2025survey,
  title={Foundation Models and Biometrics: A Survey and Outlook},
  author={Hatef Otroshi Shahreza and S{\'e}bastien Marcel},
  journal={TechRxiv preprint techrxiv:174119169.94570936},
  doi={10.36227/techrxiv.174119169.94570936/v1}
  year={2025}
}
```